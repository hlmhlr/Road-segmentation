# ğŸ›°ï¸ Drone-Based Traffic Monitoring â€“ RoI Extraction with SegRExt Models
---
## ğŸ“ Abstract

> Alongside many traditional as well as novel applications, in recent years drones have been widely adopted as remote sensing platforms for road traffic monitoring in urban areas and on highways. The problem of traffic monitoring on Region of Interest (RoI) based on drone imagery is a challenging task, especially when the surveillance drone is constantly moving. In this work, two specific sub-tasks have been addressed. The goal of the first stage is to predict the RoI in drone imagery of traffic scenes using deep learning-based approaches instead of traditional image processing; in this connection, the goal of the second task is to perform vehicle detection on the selected RoI. To ensure diversity and robustness, drone images with different altitudes, positions and viewpoints have been considered. To achieve these goals, two custom aerial datasets for RoI extraction and detection were built by collecting aerial sequences from flying Unmanned Aerial Vehicles (UAV) and by transmitting them to the base station leveraging 5G technology. Two different ad-hoc deep learning-based architectures have been designed for the RoI extraction task to maximize the accuracy and inference speed, respectively, and have been evaluated on two different datasets: a custom-built dataset and a Massachusetts roads dataset. Finally, the models providing the best performance have been combined to further improve the overall results. Experimental tests show that the proposed framework represents a promising solution for drone-based road traffic monitoring in critical areas, exploiting imagery from a variety of viewing angles and altitudes.

---

## ğŸ¯ Scope of This Repository

This repository contains only the **RoI extraction stage** of the complete pipeline. Specifically, it includes the following two custom deep learning architectures:

- **SegRExt-A**: Accuracy-optimized model
- **SegRExt-F**: Speed-optimized (Fast) model

Both models are tailored for semantic segmentation of road regions in drone-captured aerial images.

---

## ğŸ–¼ï¸ Complete pipeline and SegRExt-A Model Architecture
![resnet1](https://github.com/user-attachments/assets/ab753f95-4887-4ec9-9334-b78d2b8dd866)

## ğŸ–¼ï¸ SegRExt-F Model Architecture
> *Figure: Complete pipeline and the SegRExt-A model with ResNet bloacks to attain higher accuracies.*

![mnetv3_diagram](https://github.com/user-attachments/assets/8f7798f2-9068-418e-9b4c-25e01b4b8d7c)
> *Figure: The SegRExt-F model uses a lightweight encoder-decoder architecture with attention blocks and skip connections for fast inference on aerial imagery.*

---

## ğŸ“Š Quantitative Results

![segrext_model_performances](https://github.com/user-attachments/assets/0bedae0b-8d10-423c-bf78-d5cf592840bc)

> Quantitative analysis of RoI extractors. *SegRExt-F achieves high frame rates while maintaining competitive segmentation performance.*

## ğŸ¨ Qualitative Analysis

![segrext_model_performances_fig](https://github.com/user-attachments/assets/843c7347-a642-415f-a639-fcf40aa3b038)

> Qualitative analysis of RoI extractors. 

---
## ğŸ“ Project Structure
<pre lang="markdown"> 
â”œâ”€â”€ checkpoints/ # Saved model weights (.pth) 
â”œâ”€â”€ dataset/ # Dataset root â”‚ 
    â”œâ”€â”€ train/ â”‚ 
    â”‚ â”œâ”€â”€ image/ # Training images â”‚ 
    â”‚ â””â”€â”€ mask/ # Ground-truth masks â”‚ 
    â”œâ”€â”€ val/ â”‚ 
    â”‚ â”œâ”€â”€ image/ # Validation images â”‚ 
    â”‚ â””â”€â”€ mask/ # Validation masks â”‚ 
    â””â”€â”€ test/ 
    â”‚ â”œâ”€â”€ image/ # Test images 
    â”‚ â””â”€â”€ mask/ # Test masks (optional) 
â”œâ”€â”€ outputs/ # Predicted masks generated by test.py 
â”œâ”€â”€ logs/ # TensorBoard & txt logs â”‚ 
â”œâ”€â”€ train.py # Train / fine-tune a model 
â”œâ”€â”€ test.py # Evaluate a checkpoint or export predictions â”‚ 
â”œâ”€â”€ model.py # Vanilla UNet / CompNet definitions â”œâ”€â”€ model_mnetv3_2_ca_sa.py # MobileNetV3 backboneâ”‚ 
â”œâ”€â”€ loss.py # BCE, Dice, Tversky, etc. 
â”œâ”€â”€ Hybrid_Eloss.py # Experimental hybrid edge loss â”‚ 
â”œâ”€â”€ utils.py # Metric calculators, I/O helpers, seeds 
â”œâ”€â”€ data.py # PyTorch Dataset & Dataloader wrappers â”‚ 
â”œâ”€â”€ requirements.txt # Exact package versions 
â””â”€â”€ README.md # Youâ€™re here  </pre>

## âš™ï¸ Installation

1. **Clone the repository**

```bash
git clone https://github.com/hlmhlr/Road-segmentation.git
cd road_segmentation
```

2. **Install dependencies**

```bash
pip install -r requirements.txt
```
âœ… Tip: Use a virtual environment:

```bash
conda create -n segrext python=3.8
conda activate segrext
```

## ğŸš€ Usage

â–¶ï¸ Training the Model

```bash
python train.py \
    --path ./dataset \
    --checkpoint_path ./checkpoints/best_model.pth \
    --train_log_path ./logs/train_log.txt \
    --batch_size 1 \
    --num_epochs 50
```
Arguments:

--path: Root path to the dataset.

--checkpoint_path: Where to save the trained .pth model.

--train_log_path: File to log loss and metrics.

--batch_size: Batch size for training.

--num_epochs: Number of training epochs.



## ğŸ§ª Testing / Inference
```bash
python test.py \
    --path ./dataset \
    --checkpoint_path ./checkpoints/best_model.pth \
    --results_path ./outputs
```
Arguments:

--path: Dataset path (should include test/image and test/mask).

--checkpoint_path: Path to the trained model file.

--results_path: Folder to save the predicted masks.


## ğŸ§¾ Dataset Format
The dataset must follow the structure as mentioned in the project structre above and all images are kept as .png files.

## ğŸ“ˆ Output & Metrics
At the end of testing, the following segmentation metrics are printed and saved:

Jaccard Index (IoU)

F1 Score

Recall

Precision

Accuracy

F2 Score

Inference FPS (Frames per Second)

Outputs:
Predicted masks are saved to the outputs/mask folder.

Evaluation results are stored in a .csv file inside the results directory.

## ğŸ“Œ Notes
Images are resized to 256Ã—256 before inference.

Predicted masks are binarized with a threshold of 0.5.

The model is evaluated on test.py using ground-truth masks.

The entire code supports CUDA if a GPU is available.

## ğŸ“š Citation
Please cite our work if you use it for your research and find it useful.

@ARTICLE{9955239,
  author={Bisio, Igor and Garibotto, Chiara and Haleem, Halar and Lavagetto, Fabio and Sciarrone, Andrea},
  journal={IEEE Internet of Things Journal}, 
  title={Traffic Analysis Through Deep-Learning-Based Image Segmentation From UAV Streaming}, 
  year={2023},
  volume={10},
  number={7},
  pages={6059-6073},
}


## ğŸ“„ License
The source code is free for research and education use only. Any comercial use should get formal permission first.

---
